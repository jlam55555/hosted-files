# -*- coding: utf-8 -*-
"""ece475_proj1_lam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PMV3LP4WmX4IdSAZaE0RznrEPD2H1jLY

Jonathan Lam<br>
Prof. Keene<br>
ECE 475<br> 
Frequentist Machine Learning<br> 
9 / 10 / 20<br> 
Project 1

---

# Part 0: The model

The following classes define the regression techniques. `BaseLinearRegressionModel` includes some common utility functions for all of the regression techniques, including a `validate()` function and a `subsetSplit()` function. It is subclassed by `BasicLinearRegressionModel`, `RidgeRegressionModel`, and `LassoRegressionModel`, which implement different training techniques (overriding the `fit()` method).

## Part 0a: Base class
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn import linear_model


# base class for multi-input single-output regression
class BaseLinearRegressionModel:
  
    # features should be a ndarray NxP ndarray, labels a Nx1 ndarray,
    # featureNames should be an array of length P
    def __init__(self, features=None, labels=None, featureNames=None,
                 copySubsetsFrom=None):

        # if copySubsetsFrom is set, use this as a copy constructor,
        # don't redo subset split so that the same train/validate/test
        # subsets are available
        if copySubsetsFrom is not None:
            self._features = copySubsetsFrom._features
            self._labels = copySubsetsFrom._labels
            self._featureNames = copySubsetsFrom._featureNames
            self._subsets = copySubsetsFrom._subsets
            # don't copy validation data nor betas, these should be recalculated
            # with a new model
            return

        if features is None or labels is None or featureNames is None:
            raise Exception('missing parameters')

        self._features = features
        self._labels = labels
        self._featureNames = featureNames

        self._beta = None
        self._validationData = None

        self.subsetSplit()

    # this should be overridden using the regression strategy in subclasses
    def fit(self):
        raise Exception('cannot call fit() on abstract base regression class')

    # this very basic validation function performs a sweep and returns
    # the MSEs calculated for each beta on the validation data; the results are
    # stored in self._validationData so that they can be plotted later
    def validate(self, lbdas, betas):
        # calculate mses for all betas over the training set
        mses = np.array([self.mse(beta, subset='validation') for beta in betas])

        # store results of validation for use in graphing
        self._validationData = lbdas, mses, betas

        # return beta with minimum mse
        return betas[mses == np.min(mses), :].flatten()

    def plotValidation(self):
        lbdas, mses, betas = self._validationData
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=((15,15)))

        # plot coefficients vs. lambda
        ax1.plot(np.log(lbdas), betas[:, 1:])
        ax1.axvline(np.log(lbdas[mses == np.min(mses)]), ls='--')
        ax1.legend(list(self._featureNames) + ['optimal lambda'],
                   bbox_to_anchor=(1.05, 1), loc='upper left')
        ax1.set_title('coefficients vs. log(lambda)')
        ax1.set_ylabel('coefficients')
        ax1.set_xlabel('log(lambda)')
        ax1.grid('on')

        # plot mse vs. lambda
        ax2.plot(np.log(lbdas), mses)
        ax2.axvline(np.log(lbdas[mses == np.min(mses)]), ls='--')
        ax2.set_title('mse (on validation subset) vs. log(lambda)')
        ax2.set_ylabel('mse')
        ax2.set_xlabel('log(lambda)')
        ax2.grid('on')

        return fig

    # create a 80/10/10 train/validation/test subset split; this can be called
    # after fitting to create a new subset partitioning
    def subsetSplit(self):
        # N := total number of samples
        N, _ = self._features.shape

        # invalidate beta and validation data, since these were run on a
        # different subset partitioning
        self._beta = None
        self._validationData = None

        # splits (80/10/10)
        s1, s2 = 0.8, 0.9

        # randomly assign samples to datasets with given splits
        shufIndices = np.arange(N)
        np.random.shuffle(shufIndices)
        self._subsets = {
            'train': {
                'features': self._features[shufIndices[:int(s1 * N)]],
                'labels': self._labels[shufIndices[:int(s1 * N)]],
            },
            'validation': {
                'features': self._features[shufIndices[int(s1 * N):int(s2 * N)]],
                'labels': self._labels[shufIndices[int(s1 * N):int(s2 * N)]],
            },
            'test': {
                'features': self._features[shufIndices[int(s2 * N):]],
                'labels': self._labels[shufIndices[int(s2 * N):]],
            },
        }

    # normalize each feature in feature matrix X; z-score makes this easy
    # this is equivalent to sklearn.preprocessing.StandardScaler (I checked it
    # manually -- the output of this function matches that of StandardScaler)
    @staticmethod
    def _normalizeFeatures(X):
        result = np.zeros(X.shape)
        for i in range(X.shape[1]):
            # prevent error if stdev == 0; normalized is all zeros
            if np.std(X[:, i]) == 0:
                result[:, i] = np.zeros(X.shape[0])
                continue

            # feature = (feature - featureMean) / featureStdev (i.e., z-score)
            result[:, i] = stats.zscore(X[:, i])
        return result

    # helper function to get features and labels for a given subset;
    # returns the normalized augmented feature matrix X (with column of 1's)
    # and labels y for the given subset as a tuple; also returns
    # N, P, where N is number of training samples and P is number of features
    # (number of cols - 1, because of the column of 1's); default subset
    # is training subset
    def _subsetXy(self, subset='train'):
        # X := training feature set
        # Y := training label set
        X = self._subsets[subset]['features']
        y = self._subsets[subset]['labels']

        # N := number of training samples
        # P := number of features
        N, P = X.shape

        # scale down feature set
        X = self._normalizeFeatures(X)

        # augment X into Nx(P+1) matrix by adding a column of 1's at front
        X = np.insert(X, 0, [1] * N, axis=1)
        return X, y, N, P

    # compute and return mean-squared error of beta (on test subset)
    # uses the last computed beta by default, but can use a given beta
    # (i.e., when doing validation)
    def mse(self, beta, subset='test'):
        if beta is None and self._beta is None:
            return None

        # get test labels and normalized test subset features
        X, y, _, _ = self._subsetXy(subset)
        return np.mean((y - (X @ beta)) ** 2)

    # compute baseline mse (just a bias at the average value)
    def baselineMse(self):
        # get labels of training subset
        _, y, _, P = self._subsetXy()

        # run mse on test subset against the mean of the training subset labels
        baselineBeta = np.zeros(P + 1)
        baselineBeta[0] = np.mean(y)
        return self.mse(baselineBeta)

    # return correlation coefficient matrix of normalized features
    # this is performed on all subsets to get a sense of the population
    def corrCoef(self):
        return np.corrcoef(self._normalizeFeatures(self._features).T)

    # compute and return stderrs and z-scores for beta (on training subset)
    def zScores(self):
        beta = self._beta
        if beta is None:
            return None

        X, y, N, P = self._subsetXy()

        # estimate variance (eq. 3.8), calculate stderr and z-scores (eq. 3.12)
        variance = np.sum((y - (X @ beta)) ** 2) / (N - P - 1)
        stdErrs = np.sqrt(variance * np.diagonal(np.linalg.pinv(X.T @ X)))
        zScores = beta / stdErrs
        return stdErrs, zScores

    # beta getter
    def beta(self):
        return self._beta

"""## Part 0b: No regularization model"""

# no regularization, basic linear regression; not to be confused with
# BaseLinearRegressionModel, which is the base class for the all of the
# linear regression models here
class BasicLinearRegressionModel(BaseLinearRegressionModel):

    def fit(self):
        # get normalized features matrix and labels for training subset
        X, y, _, _ = self._subsetXy()

        # compute beta (eq. 3.6)
        self._beta = np.linalg.pinv(X.T @ X) @ X.T @ y
        return self._beta

"""## Part 0c: Ridge regression model

Regression class with ridge regularization and validation. Validation works by performing a sweep on selected values in $\lambda\in[1,1000]$.
"""

# ridge regularization model; sweeps the parameter for lambda (rather than
# using DoF)
class RidgeLinearRegressionModel(BaseLinearRegressionModel):

    def fit(self):
        # get normalized features matrix and labels for training subset
        X, y, _, P = self._subsetXy()

        # column of 1's in X to avoid penalizing the bias
        X = X[:, 1:]

        # generate lambdas, beta candidates to sweep
        lbdas = np.hstack((np.linspace(0.0001, 0.01, 100),
                           np.linspace(0.01, 1, 100),
                           np.linspace(1, 100, 100),
                           np.linspace(100, 10000, 100)))
        betaCandidates = np.array([
            # eq. 3.44: ridge regression
            np.linalg.pinv(X.T @ X + lbda * np.eye(P)) @ X.T @ y
            for lbda in lbdas
        ])

        # reinsert column of biases, estimate bias with mean of y
        betaCandidates = np.insert(betaCandidates, 0,
                                   [np.mean(y)] * len(lbdas), axis=1)

        # choose best beta by validation
        self._beta = self.validate(lbdas, betaCandidates)
        return self._beta

"""## Part 0d: Lasso regression model

Regression class with lasso regularization and validation. Uses the `sklearn.linear_model.Lasso` function on the training inputs, and sweeps $\alpha\in [0.001,0.9]$. (Here, $\alpha$ is represented with the `lbda` variable to make it more consistent with the ridge regression parameters.)
"""

class LassoLinearRegressionModel(BaseLinearRegressionModel):

    def fit(self):
        # get normalized features matrix and labels for training subset
        X, y, _, P = self._subsetXy()

        # remove leading 1's from feature matrix, sklearn.linear_model.Lasso
        # will take care of intercepts
        X = X[:,1:]

        # generate lambdas and betas to sweep over
        lbdas = np.hstack((np.linspace(0.0001, 0.01, 100),
                           np.linspace(0.01, 1, 100)))
        betaCandidates = np.zeros((len(lbdas), P+1))
        for i, lbda in enumerate(lbdas):
            model = linear_model.Lasso(alpha=lbda, fit_intercept=True)
            model.fit(X, y)
            betaCandidates[i,:] = np.hstack((model.intercept_, model.coef_))

        # choose best beta by validation
        self._beta = self.validate(lbdas, betaCandidates)
        return self._beta

"""---

# Part 1: Prostate cancer dataset

Load prostate data file as a Pandas array and do some basic cleaning.

Features:
- lcaweight: log cancer volume
- lweight: log prostate weight
- age: patient age
- lbph: log of benign prostate hyperplasia
- svi: seminal vesicle invasion
- lcp: log of capsular penetration
- gleason: Gleason score
- pgg45: percent of Gleason scores 4 or 5

Target:
- lpsa: log of prostate-specific antigen
"""

# read in pandas dataframe from csv, drop first column (indices)
df = pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data', '\t')
df = df.drop(df.columns[0], axis=1)

# ignore training label (67/30 split), we'll do our own 80/10/10 split and
# approximate the results rather than try to match the values in the textbook
df = df.drop('train', axis=1)

# get features and labels from the dataset
features = df.drop('lpsa', axis=1)
labels = df.loc[:, 'lpsa']

print('Prostate cancer dataset preview')
df

"""Load data into model, print out feature correlation."""

# create the basic linear regression model, run fit, get values
basicReg = BasicLinearRegressionModel(features.to_numpy(),
                                      labels.to_numpy(), features.columns)
print(f'baseline mse: {basicReg.baselineMse()}\n')
print('Feature correlation')
pd.DataFrame(data=np.around(basicReg.corrCoef(), 2),
              index=features.columns,
              columns=features.columns)

"""## Part 1a: No regularization"""

basicReg.fit()
print(f'mse: {basicReg.mse(basicReg.beta())}\n')
pd.DataFrame(data=np.around(np.vstack((basicReg.beta(),
                                       basicReg.zScores())),2).T,
             columns=['beta', 'stderr', 'zscore'],
             index=np.hstack((['bias'],features.columns)))

"""## Part 1b: Ridge regularization

Run the linear regression with ridge regularization. This uses the same subset partitioning as the basic linear regression without regularization.
"""

# create ridge reg. with same training/validation/test data as first regression
ridgeReg = RidgeLinearRegressionModel(copySubsetsFrom=basicReg)
ridgeReg.fit()
print(f'mse: {ridgeReg.mse(ridgeReg.beta())}\n')
pd.DataFrame(data=np.around(np.vstack((ridgeReg.beta(),
                                       ridgeReg.zScores())),2).T,
             columns=['beta', 'stderr', 'zscore'],
             index=np.hstack((['bias'],features.columns)))

ridgeReg.plotValidation().show()

"""## Part 1c: Lasso regularization

Run the linear regression with lasso regularization. This uses the same subset partitioning as the basic linear regression without regularization.
"""

# create lasso reg. with same training/validation/test data as first regression
lassoReg = LassoLinearRegressionModel(copySubsetsFrom=basicReg)
lassoReg.fit()
print(f'mse: {lassoReg.mse(lassoReg.beta())}\n')
pd.DataFrame(data=np.around(np.vstack((lassoReg.beta(),
                                       lassoReg.zScores())),2).T,
             columns=['beta', 'stderr', 'zscore'],
             index=np.hstack((['bias'],features.columns)))

lassoReg.plotValidation().show()

"""---

# Part 2: Red Wine dataset

Attribute information:

> For more information, read [Cortez et al., 2009].<br>
Input variables (based on physicochemical tests):<br>
1 - fixed acidity<br>
2 - volatile acidity<br>
3 - citric acid<br>
4 - residual sugar<br>
5 - chlorides<br>
6 - free sulfur dioxide<br>
7 - total sulfur dioxide<br>
8 - density<br>
9 - pH<br>
10 - sulphates<br>
11 - alcohol<br>
Output variable (based on sensory data):<br>
12 - quality (score between 0 and 10)<br>

Dataset source: [UCI "Wine Quality" dataset](https://archive.ics.uci.edu/ml/datasets/Wine+Quality).

The features are all quantitative and there is no missing data, so no data cleaning needs to be performed.
"""

df2 = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',';')
features2 = df2.drop(['quality'], axis=1)
labels2 = df2.loc[:, 'quality']

print('Red wine dataset preview')
df2

basicReg2 = BasicLinearRegressionModel(features2.to_numpy(),
                                       labels2.to_numpy(), features2.columns)
print(f'baseline mse: {basicReg2.baselineMse()}\n')
print('Feature correlation')
pd.DataFrame(data=np.around(basicReg2.corrCoef(), 2),
              index=features2.columns,
              columns=features2.columns)

"""## Part 2a: No regularization"""

basicReg2.fit()
print(f'mse: {basicReg2.mse(basicReg2.beta(), "test")}\n')
pd.DataFrame(data=np.around(np.vstack((basicReg2.beta(),
                                       basicReg2.zScores())),2).T,
             columns=['beta', 'stderr', 'zscore'],
             index=np.hstack((['bias'],features2.columns)))

"""## Part 2b: Ridge regularization"""

ridgeReg2 = RidgeLinearRegressionModel(copySubsetsFrom=basicReg2)
ridgeReg2.fit()
print(f'mse: {ridgeReg2.mse(ridgeReg2.beta())}\n')
pd.DataFrame(data=np.around(np.vstack((ridgeReg2.beta(),
                                       ridgeReg2.zScores())),2).T,
             columns=['beta', 'stderr', 'zscore'],
             index=np.hstack((['bias'],features2.columns)))

ridgeReg2.plotValidation().show()

"""## Part 2c: Lasso regularization"""

lassoReg2 = LassoLinearRegressionModel(copySubsetsFrom=basicReg2)
lassoReg2.fit()
print(f'mse: {lassoReg2.mse(lassoReg2.beta())}\n')
pd.DataFrame(data=np.around(np.vstack((lassoReg2.beta(),
                                       lassoReg2.zScores())),2).T,
             columns=['beta', 'stderr', 'zscore'],
             index=np.hstack((['bias'],features2.columns)))

lassoReg2.plotValidation().show()

"""---

# Part 3: Feature engineering

As a basic form of feature engineering, I'll try to add some powers (nonlinear terms) of the original features of the red wine dataset and see if this improves the results.

(I also messed around somewhat with interaction terms by summing/multiplying arbitrary features, but was unable to get any sort of consistent and significant results from this. I wasn't sure of a systematic way of displaying any interaction terms, so I left them out.)
"""

features3 = features2.copy()
labels3 = labels2.copy()

features3Squared = features3 ** 2
features3Squared.columns = [f'({name})^2' for name in features3Squared.columns]
features3Cubed = features3 ** 3
features3Cubed.columns = [f'({name})^3' for name in features3Cubed.columns]

features3All = pd.concat([features3, features3Squared, features3Cubed], axis=1)
print('Preview of engineered features')
features3All

basicReg3 = BasicLinearRegressionModel(features3All.to_numpy(),
                                       labels3.to_numpy(), features3All.columns)
print(f'baseline mse: {basicReg3.baselineMse()}\n')
print('Feature correlation')
pd.DataFrame(data=np.around(basicReg3.corrCoef(), 2),
              index=features3All.columns,
              columns=features3All.columns)

"""## Part 3a: No regularization"""

basicReg3.fit()
print(f'mse: {basicReg3.mse(basicReg3.beta(), "test")}\n')
pd.DataFrame(data=np.around(np.vstack((basicReg3.beta(),
                                       basicReg3.zScores())),2).T,
             columns=['beta', 'stderr', 'zscore'],
             index=np.hstack((['bias'],features3All.columns)))

"""## Part 3b: Ridge regularization"""

ridgeReg3 = RidgeLinearRegressionModel(copySubsetsFrom=basicReg3)
ridgeReg3.fit()
print(f'mse: {ridgeReg3.mse(ridgeReg3.beta())}\n')
pd.DataFrame(data=np.around(np.vstack((ridgeReg3.beta(),
                                       ridgeReg3.zScores())),2).T,
             columns=['beta', 'stderr', 'zscore'],
             index=np.hstack((['bias'],features3All.columns)))

ridgeReg3.plotValidation().show()

"""## Part 3c: Lasso regularization"""

lassoReg3 = LassoLinearRegressionModel(copySubsetsFrom=basicReg3)
lassoReg3.fit()
print(f'mse: {lassoReg3.mse(lassoReg3.beta())}\n')
pd.DataFrame(data=np.around(np.vstack((lassoReg3.beta(),
                                       lassoReg3.zScores())),2).T,
             columns=['beta', 'stderr', 'zscore'],
             index=np.hstack((['bias'],features3All.columns)))

lassoReg3.plotValidation().show()

"""---

# Answers to questions

*Which features did the Lasso select for you to include in your model? Do these features make sense?*

- In the prostate cancer dataset, mostly only the lcp and age features were suppressed with the Lasso. I don't know much about prostate cancer, so I can't interpret the results of this too much.
- In the red wine dataset, there were a few coefficients that were very low even in the no-regularization case, such as fixed acidity, density, and pH. The factors that were most severely suppressed were the total and fixed sulfur concentrations, as well as the sulfates feature. I don't know much about wine either, but it makes sense that people are not looking for the amount of sulfur in their wine when considering whether it's good or not.

*Compute the MSE on the training dataset and the test dataset for all methods and comment on the results. Compare this MSE to a baseline MSE.*

In both datasets, the baseline MSE was higher than the MSE calculated for any of the regression methods, as expected. See the chart following this section.

*Stretch goal: Add nonlinear and interaction terms to your dataset and try to improve the performance. Are you able to do so?*

I tried adding arbitrary interaction terms, but without knowing much about what makes quality wine I was unable to find a combination that consistently produced significant results (and I wasn't able to find a way to do this systematically). The above example shows adding nonlinear terms (squared and cubed values of the original sample dataset). When doing this:

- The baseline MSE stayed around the same value ($\approx$0.5 to 0.7).
- The regression MSEs were a little lower than without these extra features ($\approx$0.3 to 0.4, rather than $\approx 0.4$ to 0.5 without the extra features).
- The regularized models don't seem to outperform the no-regularization model by any significant amount.

Thus, even by this very basic feature engineering (adding powers of features), there seems to be a small but noticeable improvement in overall MSEs.
"""

print('MSE summary')
mses = [[basicReg.baselineMse(), basicReg.mse(basicReg.beta()),
         ridgeReg.mse(ridgeReg.beta()), lassoReg.mse(lassoReg.beta())],
        [basicReg2.baselineMse(), basicReg2.mse(basicReg2.beta()),
         ridgeReg2.mse(ridgeReg2.beta()), lassoReg2.mse(lassoReg2.beta())],
        [basicReg3.baselineMse(), basicReg3.mse(basicReg3.beta()),
         ridgeReg3.mse(ridgeReg3.beta()), lassoReg3.mse(lassoReg3.beta())]]
pd.DataFrame(data=mses,
             columns=['baseline','no reg.','ridge','lasso'],
             index=['prostate cancer', 'red wine', 'red wine feature eng.'])

"""---

# Other comments

- There was a lot of variation between runs, so there were often runs where ridge and/or lasso did worse than the no-regularization case. Given the small number of samples (small training set, validation set, and testing set all are not helpful) and a single validation (rather than k-fold cross validation), this is somewhat expected.
- There were even some times that the baseline performed better than the regressions, which made me very skeptical; however, I verified the numbers in some of these cases and conclude that this is due to the wide variation between randomly-selected train and test datasets (again due to the small size), not due to a problem in the model.
- On some runs, the validation gave a minimum MSE when $\lambda=0$ (or $\alpha=0$). Again, this is probably due to the small sample size.
"""